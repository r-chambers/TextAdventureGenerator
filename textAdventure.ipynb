{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-chambers/TextAdventureGenerator/blob/main/textAdventure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2gtWUd60Hhb"
      },
      "source": [
        "# Text Adventure LLM pipline\n",
        "\n",
        "### To run this notebook, you will need access to these folders:\n",
        "https://drive.google.com/drive/folders/1FqYtfJ9-Q7q2G8PB6T-yAPa2k6TZzpb5?usp=drive_link\n",
        "https://drive.google.com/drive/folders/1TemGXJ3Uv5KsLli4iOuCowNAxjlbqxcK?usp=drive_link\n",
        "\n",
        "Please copy them onto your drives.\n",
        "\n",
        "### Participants\n",
        "Rachel Chambers\n",
        "\n",
        "Eliot Pearson\n",
        "\n",
        "Naomi Tack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iFbYT_lm0Hhc"
      },
      "outputs": [],
      "source": [
        "# Install cell for Collab\n",
        "!pip install -qU \\\n",
        "  transformers==4.31.0 \\\n",
        "  sentence-transformers==2.2.2 \\\n",
        "  pinecone-client==2.2.2 \\\n",
        "  datasets==2.14.0 \\\n",
        "  accelerate==0.21.0 \\\n",
        "  einops==0.6.1 \\\n",
        "  langchain==0.0.240 \\\n",
        "  xformers==0.0.20 \\\n",
        "  bitsandbytes==0.41.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52qcexte0Hhe",
        "outputId": "a4d1c456-3374-433a-a531-7c2a5db3c385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import cell for Collab\n",
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import BertTokenizer, EncoderDecoderModel\n",
        "from google.colab import drive\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import pinecone\n",
        "import time\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aFpNhzyB0Hhe"
      },
      "outputs": [],
      "source": [
        "# Rachel's Code\n",
        "class GraphModel:\n",
        "  def __init__(self):\n",
        "\n",
        "    # Prompt for the BERT model\n",
        "    self.BASE_GRAPH_CMD = \"What would happen to the following graph given the provided command? Generate a new room name for these commands for the ['you' 'in', 'location'] phrase: north, east, south and west.\"\n",
        "\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-medium\")\n",
        "    self.model = EncoderDecoderModel.from_pretrained(\"/content/drive/My Drive/TextAdventureModel/model_medium_beam4\")\n",
        "\n",
        "    # Put model on GPU if it's available\n",
        "    if cuda.is_available():\n",
        "      self.model = self.model.to(\"cuda\")\n",
        "\n",
        "  # Clean up the returned graph string a little\n",
        "  def clean_graph_string(self, graph_str):\n",
        "    # If there are spaces between the first and last [ [ ] ] then remove them\n",
        "    if graph_str[1] == \" \":\n",
        "      graph_str = graph_str[:1] + graph_str[2:]\n",
        "\n",
        "    if graph_str[-2] == \" \":\n",
        "      graph_str = graph_str[:-2] + graph_str[-1:]\n",
        "\n",
        "    # Replace all \" with ' as that is what the data that trained the model used\n",
        "    graph_str = graph_str.replace(\"\\\"\", \"'\")\n",
        "\n",
        "    return graph_str\n",
        "\n",
        "  def generate_next_graph(self, command, graph):\n",
        "    # Get the input, inputs ids and attention mask\n",
        "    input = self.BASE_GRAPH_CMD  + \" command: \" + command + \" \" + \"graph: \" + graph\n",
        "\n",
        "    print(\"INPUT TO MODEL: \", input)\n",
        "\n",
        "    inputs = self.tokenizer(input, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs.attention_mask\n",
        "\n",
        "    # If we have a GPU available then put all the data on it\n",
        "    if cuda.is_available():\n",
        "      input_ids = input_ids.to(\"cuda\")\n",
        "      attention_mask = attention_mask.to(\"cuda\")\n",
        "\n",
        "    # Generate outout\n",
        "    outputs = self.model.generate(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    output = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    output_str = output[0]\n",
        "    print(\"OUTPUT STRING:\", output_str)\n",
        "\n",
        "    return self.clean_graph_string(output_str)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Eliot's Code - not connected to BERT or RAG+LLaMA\n",
        "```\n",
        "JUMP = \"jump\"\n",
        "\n",
        "file = open(\"/content/drive/MyDrive/NLP_json/examples_of_starts_of_games_PREPENDED_COMMANDS.txt\", \"r\")\n",
        "\n",
        "data = json.load(file) # loading the json for parsing\n",
        "file.close()\n",
        "\n",
        "games = [game for game in data]\n",
        "\n",
        "random_game = random.choice(games)\n",
        "graph_list = data[random_game]['graph']\n",
        "setting = data[random_game]['beginning']\n",
        "prev_graph = graph_list # no command -> rachel's model\n",
        "\n",
        "#print(prev_graph) # for demonstration purposes only\n",
        "\n",
        "# gameplay loop, type quit to quit\n",
        "command = \" \"\n",
        "condition = False\n",
        "\n",
        "while condition != True:\n",
        "\n",
        "    command = input(\"Please enter a command: \")\n",
        "\n",
        "    if command == \"quit\":\n",
        "        condition = True\n",
        "\n",
        "    if JUMP in command:\n",
        "        print(\"You jumped 15ft into the air! But nothing happened...\")\n",
        "\n",
        "\n",
        "    print('\\n') # for demonstation purposes only\n",
        "    # examine = abbrvs.get(\"x\")\n",
        "    formatted_command = [\"command\", \"is\", command] # prepend this formatted command to graph and then send to naomi's model\n",
        "\n",
        "    # Call Rachel's code here (?)\n",
        "\n",
        "\n",
        "    new_graph_list = graph_list.copy()\n",
        "    new_graph_list.pop(0)\n",
        "    new_graph_list.insert(0, formatted_command)\n",
        "\n",
        "    graph_string = [str(element) for element in new_graph_list]\n",
        "\n",
        "    graph_string = '['\n",
        "    for cmd in graph_list:\n",
        "      graph_string += '['\n",
        "      for element in cmd:\n",
        "       graph_string += element + \" \"\n",
        "      graph_string += '], '\n",
        "    graph_string += ']'\n",
        "\n",
        "    # call this setting, not prompt\n",
        "    #setting = getNextPrompt(BASE_SYS_CMD, command_history, graph_string)\n",
        "    print(graph_string) # for demonstration purposes only\n",
        "    print(graph_list)\n",
        "    print(new_graph_list)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BS8pJ5ngO8_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliot's new code (Changed by Rachel to have strings instead of graphs)\n",
        "```\n",
        "#print(prev_graph) # for demonstration purposes only\n",
        "\n",
        "# gameplay loop, type quit to quit\n",
        "command = \" \"\n",
        "condition = False\n",
        "\n",
        "while condition != True:\n",
        "    print(setting)\n",
        "    command = input(\"Please enter a command: \")\n",
        "\n",
        "    if command == \"quit\":\n",
        "        condition = True\n",
        "\n",
        "    if JUMP in command:\n",
        "        print(\"You jumped 15ft into the air! But nothing happened...\")\n",
        "\n",
        "\n",
        "    print('\\n') # for demonstation purposes only\n",
        "\n",
        "    formatted_command = \"['command', 'is', '\" + command + \"'], \" # prepend this formatted command to graph and then send to naomi's model\n",
        "    print(\"cmd: \", formatted_command)\n",
        "    print(\"graph: \", graph)\n",
        "    # Call Rachel's code\n",
        "    graph = graph_model_debug.generate_next_graph(command, graph)\n",
        "\n",
        "    # Insert command\n",
        "    graph = graph[:1] + formatted_command + graph[1:]\n",
        "    print(\"new graph:\", graph)\n",
        "    print(type(graph))\n",
        "\n",
        "    # Get the new setting\n",
        "    # setting = getNextPrompt(BASE_SYS_CMD, command_history, graph)\n",
        "```"
      ],
      "metadata": {
        "id": "ZNV87Tm5PLX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mhw9deCo0Hhf"
      },
      "outputs": [],
      "source": [
        "# Naomi's Code\n",
        "\n",
        "class RAG_LLaMA():\n",
        "  rag_llm = None\n",
        "  command_history = \"\"\n",
        "  BASE_SYS_CMD = \"<s>[INST] <<SYS>> Convert graphs into the output of a text adventure game, like Zork. Be artful and expressive. Always use second person. Do not give the user any choices about what to do next. Do not tell the user how to enter the next command. The first part of the graph will be the command in the format 'command', 'is', '{{ command }}'. For the 'use {{ item }}' command, describe the user using the item. For the 'take {{ item }}' command, describe the user taking the item. For the 'talk to {{ person }}' command, describe the user talking to the person and the conversation they have. The graph will have 'you', 'in', {{ room name }}' representing the room the player is in. The graph will also have items in the room, represented by '{{ item name }}', 'in', '{{ room name }}'. The player will also have inventory items, represented by 'you', 'have', '{{ item name }}'. Do not mention the inventory items in the text. If the player does not have an item in their inventory, then they can't use that item via the 'use {{ item }}' command. Finally, exits to the current room will be in the format '{{ room name }}', 'is', ' {{ direction }}'. Mention all the exits of a room and their directions.  Here are examples of converting graphs to output: graph: [['command', 'is', 'examine security gate lights'], ['pile of yellowed paper', 'in', 'card catalog'], ['you', 'in', 'Lobby'], ['private door', 'in', 'Lobby'], ['Pieces-Parts', 'in', 'Lobby'], ['circulation desk', 'in', 'Lobby'], ['card catalog drawer', 'in', 'Lobby'], ['circulation desk attendant', 'in', 'Lobby'], ['security gate lights', 'in', 'Lobby'], ['Ground Floor Stacks', 'is', 'west']]output: The gates are made of gunmetal grey plastic, and a set of little red lights on top seem to watch you menacingly.graph: [['command', 'is', 'west'], ['you', 'in', 'Road'], ['you', 'have', 'mysterious vial'], ['sky', 'in', 'Road'], ['crack', 'in', 'Road'], ['Charles Bristow', 'in', 'Road'], ['A Dark Hallway', 'is', 'north'], ['Public Square', 'is', 'east']]output: Public SquareThere is a large public square here, surrounded by the same strange elliptical buildings on all sides except to the east, where a high wall built of massive sandstone blocks stands. There is a road to the west leading deeper into the city. Against the sky you see a high tower to the northeast. The only trace of life comes from the south where a road leads to what appears to be a temple. You can see Charles Bristow here.graph: [['command', 'is', 'take torch'], ['you', 'in', 'Troll '], ['you', 'have', 'torch'], ['you', 'have', 'platinum bar'], ['you', 'have', 'broken lantern'], ['you', 'have', 'crystal skull'], ['Cellar', 'is', 'south']]output: Taken.graph: [['command', 'is', 'smash seal'], ['you', 'have', 'fine Pentarian sword'], ['you', 'in', 'Castle Entrance'], ['Castle', 'is', 'north']]output: Your fist smashes the ward, shattering it into a cloud of shimmering dust. <</SYS>> {{ [['command', 'is', 'examine map'], ['you', 'in', 'Captain's Cabin'], ['map', 'in', 'Captain's Cabin'], ['Captain's chair', 'in', 'Captain's Cabin'], ['painting', 'in', 'Captain's cabin'], ['desk', 'in', 'Captain's cabin'], ['Deck', 'is', 'east']] }} [/INST] </s>\"\n",
        "\n",
        "  # This takes about 3-9 mins to run :P\n",
        "  def __init__(self):\n",
        "    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "    embed_model = HuggingFaceEmbeddings(\n",
        "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "        model_kwargs={'device': device},\n",
        "        encode_kwargs={'device': device, 'batch_size': 32}\n",
        "    )\n",
        "\n",
        "    # drive.mount('/content/drive')\n",
        "    # NOTE: This needs to be change to where you have saved the parsed transcripts\n",
        "    with open(\"/content/drive/MyDrive/NLP_Json/parsed_transcripts_FINAL_12-10.json\", \"r\") as f:\n",
        "      game_data = json.loads(f.read())\n",
        "\n",
        "    stories =[]\n",
        "    titles = list(game_data.keys())\n",
        "    for title in game_data.keys():\n",
        "      story=\"<s>\"\n",
        "      for turn in game_data[title]:\n",
        "        story+=\" \"+str(turn['graph'])+\" \"+turn['output']\n",
        "      story+=\"</s>\"\n",
        "      stories.append(story)\n",
        "\n",
        "    embeddings = embed_model.embed_documents(stories)\n",
        "\n",
        "    print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
        "        f\"a dimensionality of {len(embeddings[0])}.\")\n",
        "\n",
        "    # NOTE: This should work , if not ask Naomi\n",
        "    # get API key from app.pinecone.io and environment from console\n",
        "    pinecone.init(\n",
        "        api_key=os.environ.get('4f04a986-c588-45b1-a6a6-e175a3faaa82') or '4f04a986-c588-45b1-a6a6-e175a3faaa82',\n",
        "        environment=os.environ.get('gcp-starter') or 'gcp-starter'\n",
        "    )\n",
        "\n",
        "    index_name = 'llama-2-rag'\n",
        "\n",
        "    if index_name not in pinecone.list_indexes():\n",
        "      pinecone.create_index(\n",
        "          index_name,\n",
        "          dimension=len(embeddings[0]),\n",
        "          metric='cosine'\n",
        "      )\n",
        "      # wait for index to finish initialization\n",
        "      while not pinecone.describe_index(index_name).status['ready']:\n",
        "          time.sleep(1)\n",
        "\n",
        "    index = pinecone.Index(index_name)\n",
        "    print(index.describe_index_stats())\n",
        "\n",
        "    # Only add if needed\n",
        "    if index.describe_index_stats()['total_vector_count'] == 0:\n",
        "      print(\"Adding to vector store\")\n",
        "      batch_size = 32\n",
        "      for i in range(0, len(stories), batch_size):\n",
        "          i_end = min(len(stories), i+batch_size)\n",
        "          batch = stories[i:i_end]\n",
        "          ids = [f\"{i}\" for i, x in enumerate(batch)]\n",
        "          texts = [x for i, x in enumerate(batch)]\n",
        "          embeds = embed_model.embed_documents(texts)\n",
        "          # get metadata to store in Pinecone\n",
        "          metadata = [\n",
        "              {'ids': i,\n",
        "              'title': titles[i]} for i, x in enumerate(batch)\n",
        "          ]\n",
        "          # add to Pinecone\n",
        "          index.upsert(vectors=zip(ids, embeds, metadata))\n",
        "\n",
        "      index = pinecone.Index(index_name)\n",
        "      print(index.describe_index_stats())\n",
        "\n",
        "    model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "\n",
        "    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "    # set quantization configuration to load large model with less GPU memory\n",
        "    # this requires the `bitsandbytes` library\n",
        "    bnb_config = transformers.BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=bfloat16\n",
        "    )\n",
        "\n",
        "    # begin initializing HF items, need auth token for these\n",
        "    hf_auth = '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'\n",
        "    model_config = transformers.AutoConfig.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "        config=model_config,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map='auto',\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "    model.eval()\n",
        "    print(f\"Model loaded on {device}\")\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "      model_id,\n",
        "      use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    generate_text = transformers.pipeline(\n",
        "      task='text-generation',\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      return_full_text=True,  # langchain expects the full text\n",
        "      # we pass model parameters here too\n",
        "      temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "      max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "      repetition_penalty=1.1  # without this output begins repeating\n",
        "    )\n",
        "\n",
        "    text_field = 'text'  # field in metadata that contains text content\n",
        "\n",
        "    vectorstore = Pinecone(\n",
        "        index, embed_model.embed_query, text_field\n",
        "    )\n",
        "\n",
        "    rag_pipeline = RetrievalQA.from_chain_type(\n",
        "        llm=HuggingFacePipeline(pipeline=generate_text), chain_type='stuff',\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "\n",
        "    self.rag_llm = rag_pipeline\n",
        "    return\n",
        "\n",
        "\n",
        "  def getNextPrompt(self, current_graph):\n",
        "      # Construct the prompt and get the input from the model\n",
        "      prompt = self.BASE_SYS_CMD + self.command_history + \" <s> [INST] \" + current_graph + \" [/INST] \"\n",
        "      result = self.rag_llm(prompt)\n",
        "\n",
        "      # Update the command history\n",
        "      # NEED to end with </s> to close out previous output\n",
        "      self.command_history += \"</s> <s> [INST] \" + current_graph + \" [/INST] \" + result['result'] + \" </s>\"\n",
        "\n",
        "      # Return the prompt\n",
        "      return result['result']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqEsv9BGcyGB"
      },
      "outputs": [],
      "source": [
        "# Text Adventure Class - to be used enenvtually after debugging\n",
        "\n",
        "# Function to generate the next prompt\n",
        "class TextAdventure():\n",
        "  # Rachels's Variables\n",
        "  graph_model = None\n",
        "  # Naomi's Variables\n",
        "  rag_llm = None\n",
        "\n",
        "  def __init__(self):\n",
        "      # Calls the respective initalization functions to load the models\n",
        "      self.rag_llm = RAG_LLaMA()\n",
        "      self.graph_model = GraphModel()\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "\n",
        "    file = open(\"/content/drive/MyDrive/NLP_Json/examples_of_starts_of_games_PREPENDED_COMMANDS.txt\", \"r\")\n",
        "\n",
        "    data = json.load(file) # loading the json for parsing\n",
        "    file.close()\n",
        "\n",
        "    games = [game for game in data]\n",
        "\n",
        "    random_game = random.choice(games)\n",
        "    graph = str(data[random_game]['graph'])\n",
        "    setting = data[random_game]['beginning']\n",
        "\n",
        "    # gameplay loop, type quit to quit\n",
        "    command = \" \"\n",
        "    condition = False\n",
        "\n",
        "    while condition != True:\n",
        "        print(setting)\n",
        "        command = input(\"Please enter a command: \")\n",
        "\n",
        "        if command == \"quit\":\n",
        "            condition = True\n",
        "\n",
        "        if \"jump\" in command:\n",
        "            print(\"You jumped 15ft into the air! But nothing happened...\")\n",
        "\n",
        "\n",
        "        print('\\n') # for demonstation purposes only\n",
        "        # examine = abbrvs.get(\"x\")\n",
        "        formatted_command = \"['command', 'is', '\" + command + \"'], \" # prepend this formatted command to graph and then send to naomi's model\n",
        "\n",
        "        # Get new knowledge graph\n",
        "        graph = self.graph_model.generate_next_graph(command, graph)\n",
        "\n",
        "        # Insert command\n",
        "        graph = graph[:1] + formatted_command + graph[1:]\n",
        "\n",
        "        # Get the new setting\n",
        "        setting = self.rag_llm.getNextPrompt(graph)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "28e71dd1f9234eceb4f39484dcbfe2f0",
            "23837c5400374faab128a09a25f2dd88",
            "5d292aa95aa4445283e87b38074be34c",
            "28c90f0a31fe4dafa20d740154e4d8b7",
            "4050695d0aec4b38bf73d7f71caebf80",
            "a2008e0ede8640e2b6a5760b20c778cd",
            "095f4724ef0c488d9e2ee2574c66055e",
            "32b15f62879c4b04938fe8a4bede446f",
            "ef5f236a7d75433b9f128e99b0bd03d6",
            "ce7809033fec44e38aa3ff708178baec",
            "9eab19c5e890493a8d8c0624070a3ec7"
          ]
        },
        "id": "-4WAjoZvM1Iv",
        "outputId": "de74f9c0-e4a1-4d54-b756-1f93d0e8b96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 830 doc embeddings, each with a dimensionality of 384.\n",
            "{'dimension': 384,\n",
            " 'index_fullness': 0.00032,\n",
            " 'namespaces': {'': {'vector_count': 32}},\n",
            " 'total_vector_count': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28e71dd1f9234eceb4f39484dcbfe2f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        }
      ],
      "source": [
        "our_text_adventure = TextAdventure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bgm-MozIOQxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "567c7fa7-f34c-48b5-ef1e-4413cf99ec7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post Office\n",
            "This is the lobby of the Festeron Post Office. The walls are lined with small, private mailboxes and wanted posters. A service counter runs along the entire length of the room.\n",
            "\n",
            "Your boss, Mr. Crisp, is behind the service counter reading other people's postcards.\n",
            "\n",
            "Mr. Crisp hides the postcards away as you enter. \"Where have you been?\" he barks angrily. \"Daydreaming again, eh? I've been looking everywhere for you!\"\n",
            "\n",
            "(Your score just went up by 1 point! Your total score is 1 out of 100.)\n",
            "\n",
            "What next?\n",
            "\n",
            "Post Office\n",
            "This is the lobby of the Festeron Post Office. The walls are lined with small, private mailboxes and wanted posters. A service counter runs along the entire length of the room.\n",
            "\n",
            "Your boss, Mr. Crisp, is watching you impatiently.\n",
            "\n",
            "Mr. Crisp reaches under the service counter and pulls out a mysterious envelope. \"We just got this Special Delivery,\" he snarls, tossing it onto the service counter. \"I want you to drop it off right away. That means NOW!\"\n",
            "\n",
            "What next?\n",
            "\n",
            "Please enter a command: talk to mr crisp\n",
            "\n",
            "\n",
            "INPUT TO MODEL:  What would happen to the following graph given the provided command? Generate a new room name for these commands for the ['you' 'in', 'location'] phrase: north, east, south and west. command: talk to mr crisp graph: [['leaflet', 'in', 'mailbox'], ['Mr. Crisp', 'in', 'Post Office'], ['mailbox', 'in', 'Post Office'], ['Post Office', 'south', 'Hilltop'], ['you', 'in', 'Post Office'], ['service counter', 'in', 'Post Office'], ['wanted poster', 'in', 'Post Office'], ['Hilltop', 'is', 'north'], ['command', 'is', 'load wishbringer']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT STRING: [ ['you ','in ','hilltop'], ['outside cemetery ','is ','west'], ['post office ','is ','south'], ['outside cottage ','is ','east'] ]\n",
            "\n",
            "You find yourself standing in the quaint Hilltop village, surrounded by picturesque cottages and bustling with activity. As you look around, you notice several intriguing locations to explore. To your west lies the Outside Cemetery, a peaceful final resting place for the town's former residents. South of you stands the Post Office, a hub of communication and commerce. And to the east, you spot the Outside Cottage, a charming abode nestled among the rolling hills. Which location would you like to investigate further?\n",
            "\n",
            "Please respond with the desired command (e.g., \"use map,\" \"take key,\" or \"talk to Mr. Crisp\").\n",
            "Please enter a command: talk to mr crisp\n",
            "\n",
            "\n",
            "INPUT TO MODEL:  What would happen to the following graph given the provided command? Generate a new room name for these commands for the ['you' 'in', 'location'] phrase: north, east, south and west. command: talk to mr crisp graph: [['command', 'is', 'talk to mr crisp'], ['you ','in ','hilltop'], ['outside cemetery ','is ','west'], ['post office ','is ','south'], ['outside cottage ','is ','east']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT STRING: [ ['you ','in ','hilltop'], ['outside cemetery ','is ','west'], ['post office ','is ','south'], ['outside cottage ','is ','east'] ]\n",
            "\n",
            "\n",
            "As you stroll through the picturesque Hilltop village, you come across a friendly local named Mr. Crisp. He's sitting on a bench outside his cozy cottage, enjoying the warm sunshine and chirping birds.\n",
            "\n",
            "\"Good day, young traveler!\" Mr. Crisp greets you with a smile. \"What brings you to our humble village?\"\n",
            "\n",
            "You can choose to:\n",
            "\n",
            "1. Ask Mr. Crisp about the local history and legends.\n",
            "2. Inquire about the mysterious Outside Cemetery.\n",
            "3. Discuss the bustling Post Office and its services.\n",
            "4. Explore the charming Outside Cottage.\n",
            "\n",
            "Which topic would you like to pursue? Please respond with the corresponding number.\n",
            "Please enter a command: inquire about mysterious outside cemetery\n",
            "\n",
            "\n",
            "INPUT TO MODEL:  What would happen to the following graph given the provided command? Generate a new room name for these commands for the ['you' 'in', 'location'] phrase: north, east, south and west. command: inquire about mysterious outside cemetery graph: [['command', 'is', 'talk to mr crisp'], ['you ','in ','hilltop'], ['outside cemetery ','is ','west'], ['post office ','is ','south'], ['outside cottage ','is ','east']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain.vectorstores.pinecone:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT STRING: [ ['you ','in ','hilltop'], ['outside cemetery ','is ','west'], ['post office ','is ','south'], ['outside cottage ','is ','east'] ]\n",
            "\n",
            "\n",
            "Mr. Crisp leans in, lowering his voice as if sharing a secret. \"Ah, yes... the Outside Cemetery. A most peculiar place, that one. Folks 'round here whisper tales of ghostly apparitions and unexplained occurrences. Some even claim to have seen shadowy figures lurking within the tombstones.\"\n",
            "\n",
            "He pauses, glancing around nervously before continuing, \"But I reckon that's just old wives' tales. After all, there's no such thing as ghosts, right?\" Mr. Crisp winks, his eyes twinkling with mischief. \"Still, if you're feeling brave, you might want to pay a visit. Just be careful, mind you. The villagers say that those who venture too close to the graves at night may hear eerie whispers and feel an icy presence watching them.\"\n",
            "\n",
            "Do you:\n",
            "\n",
            "1. Thank Mr. Crisp for the information and proceed to the Outside Cemetery.\n",
            "2. Ask Mr. Crisp more questions about the cemetery and its legends.\n",
            "3. Visit the Post Office instead to gather more information.\n",
            "4. Go back to exploring the Outside Cottage.\n",
            "\n",
            "Choose your response.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-757bb326a1ea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_text_adventure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-befc6a597533>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter a command: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "our_text_adventure.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28e71dd1f9234eceb4f39484dcbfe2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23837c5400374faab128a09a25f2dd88",
              "IPY_MODEL_5d292aa95aa4445283e87b38074be34c",
              "IPY_MODEL_28c90f0a31fe4dafa20d740154e4d8b7"
            ],
            "layout": "IPY_MODEL_4050695d0aec4b38bf73d7f71caebf80"
          }
        },
        "23837c5400374faab128a09a25f2dd88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2008e0ede8640e2b6a5760b20c778cd",
            "placeholder": "​",
            "style": "IPY_MODEL_095f4724ef0c488d9e2ee2574c66055e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5d292aa95aa4445283e87b38074be34c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b15f62879c4b04938fe8a4bede446f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef5f236a7d75433b9f128e99b0bd03d6",
            "value": 3
          }
        },
        "28c90f0a31fe4dafa20d740154e4d8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce7809033fec44e38aa3ff708178baec",
            "placeholder": "​",
            "style": "IPY_MODEL_9eab19c5e890493a8d8c0624070a3ec7",
            "value": " 3/3 [02:05&lt;00:00, 38.77s/it]"
          }
        },
        "4050695d0aec4b38bf73d7f71caebf80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2008e0ede8640e2b6a5760b20c778cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095f4724ef0c488d9e2ee2574c66055e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32b15f62879c4b04938fe8a4bede446f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5f236a7d75433b9f128e99b0bd03d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce7809033fec44e38aa3ff708178baec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eab19c5e890493a8d8c0624070a3ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}